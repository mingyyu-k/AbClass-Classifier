from sklearn.model_selection import KFold, cross_validate
from sklearn.neighbors import KNeighborsClassifier as KNNC
from sklearn.linear_model import LogisticRegression as LogiR 
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.ensemble import StackingClassifier
from sklearn import svm
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, precision_score, f1_score
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')

data = pd.read_csv(r"C:\Users\final.csv", encoding='gbk')

data.drop("V1", axis=1, inplace=True)
numeric_columns = data.columns[1:] 
data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')
data.dropna(inplace=True)
data['label'] = data['label'].astype('float32')
X = data.drop('label', axis=1).values
y = data['label'].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)


def fusion_estimators(clf):
    cv = KFold(n_splits=5, shuffle=True, random_state=1412) 
    results = cross_validate(clf, X_train, Y_train
                             , cv=cv
                             , scoring="accuracy"  
                             , n_jobs=-1  
                             , return_train_score=True
                             , verbose=False) 
    test = clf.fit(X_train, Y_train).score(X_test, Y_test)
    print("train_score:{}".format(results["train_score"].mean())
          , "\n cv_mean:{}".format(results["test_score"].mean())
          , "\n test_score:{}".format(test)
          )

def individual_estimators(estimators):
    for estimator in estimators:
        cv = KFold(n_splits=5, shuffle=True, random_state=1400)
        results = cross_validate(estimator[1], X_train, Y_train, cv=cv,
                                 scoring=("accuracy", "precision_weighted", "recall_weighted", "f1_weighted"),
                                 n_jobs=-1, return_train_score=True, verbose=False)
        estimator[1].fit(X_train, Y_train)
        Ypred = estimator[1].predict(X_test)

        test_accuracy = accuracy_score(Y_test, Ypred)
        test_precision = precision_score(Y_test, Ypred, average='weighted', zero_division=0)
        test_recall = recall_score(Y_test, Ypred, average='weighted', zero_division=0)
        test_f1 = f1_score(Y_test, Ypred, average='weighted', zero_division=0)

        print(f'{estimator[0]}: \n'
              f' Test Accuracy: {test_accuracy:.3f}\n'
              f' Test F1: {test_f1:.3f}\n'
              f' Test Precision: {test_precision:.3f}\n'
              f' Test Recall: {test_recall:.3f}\n')


clf1 = LogiR(max_iter=3000, C=0.1, random_state=1412, n_jobs=8) 
clf2 = RFC(n_estimators=165, max_features="sqrt", max_depth=4, min_samples_leaf=4, random_state=1412,
           n_jobs=8)  
clf3 = svm.SVC(C=10)
clf4 = KNNC(n_neighbors=10, n_jobs=8)

estimators =[("Logistic Regression",clf1), ("RandomForest", clf2),("svm",clf3), ("KNN",clf4)]

final_estimator = RFC(n_estimators=100
                      , min_impurity_decrease=0.0025
                      , random_state=420, n_jobs=8)

clf = StackingClassifier(estimators=estimators
                         , final_estimator=final_estimator  
                         , n_jobs=8)

fusion_estimators(clf)

Y_pred = clf.predict(X_test)

class_names = ['anti-dengue', 'anti-influenza', 'anti-tetanus', 'anti-sars-cov2', 'anti-Tuberculosis']

individual_estimators(estimators)

import shap
import matplotlib.pyplot as plt


explainer = shap.KernelExplainer(clf.predict_proba, X_test)
shap_values = explainer.shap_values(X_test)

feature_names = data.columns[1:]

shap.summary_plot(shap_values, X_test, feature_names=feature_names)

plt.show()
